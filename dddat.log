"backend/llm/brainpicking.py" ```import os
from typing import Any, Dict, List
from models.settings import LLMSettings
from langchain.chains import ConversationalRetrievalChain, LLMChain
from langchain.chains.question_answering import load_qa_chain
from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser
from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE
from langchain.chat_models import ChatOpenAI, ChatVertexAI
from langchain.chat_models.anthropic import ChatAnthropic
from langchain.docstore.document import Document
from langchain.embeddings.base import Embeddings
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.llms import GPT4All
from langchain.llms.base import LLM
from langchain.memory import ConversationBufferMemory
from langchain.vectorstores import SupabaseVectorStore
from llm.prompt import LANGUAGE_PROMPT
from llm.prompt.CONDENSE_PROMPT import CONDENSE_QUESTION_PROMPT
from models.chats import ChatMessage
from models.settings import BrainSettings
from pydantic import BaseModel, BaseSettings
from supabase import Client, create_client
from vectorstore.supabase import CustomSupabaseVectorStore
from logger import get_logger

logger = get_logger(__name__)

class AnswerConversationBufferMemory(ConversationBufferMemory):
    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:
        # Overriding the save_context method of the parent class
        return super(AnswerConversationBufferMemory, self).save_context(
            inputs, {"response": outputs["answer"]}
        )
def get_chat_history(inputs) -> str:
    res = []
    for human, ai in inputs:
        res.append(f"{human}:{ai}\n")
    return "\n".join(res)


class BrainPicking(BaseModel):
    llm_name: str = "gpt-3.5-turbo"
    settings = BrainSettings()
    llm_config = LLMSettings()
    embeddings: OpenAIEmbeddings = None
    supabase_client: Client = None
    vector_store: CustomSupabaseVectorStore = None
    llm: LLM = None
    question_generator: LLMChain = None
    doc_chain: ConversationalRetrievalChain = None

    class Config:
        arbitrary_types_allowed = True

    def init(self, model: str, user_id: str) -> "BrainPicking":
        self.embeddings = OpenAIEmbeddings(openai_api_key=self.settings.openai_api_key)
        self.supabase_client = create_client(
            self.settings.supabase_url, self.settings.supabase_service_key
        )
        self.vector_store = CustomSupabaseVectorStore(
            self.supabase_client,
            self.embeddings,
            table_name="vectors",
            user_id=user_id,
        )
                    
        self.llm = self._determine_llm(
            private_model_args={
                "model_path": self.llm_config.model_path,
                "n_ctx": self.llm_config.model_n_ctx,
                "n_batch": self.llm_config.model_n_batch,
            },
            private=self.llm_config.private,
            model_name=self.llm_name,
        )
        self.question_generator = LLMChain(
            llm=self.llm, prompt=CONDENSE_QUESTION_PROMPT
        )
        self.doc_chain = load_qa_chain(self.llm, chain_type="stuff")
        return self

    def _determine_llm(
        self, private_model_args: dict, private: bool = False, model_name: str = None
    ) -> LLM:
        if private:
            model_path = private_model_args["model_path"]
            model_n_ctx = private_model_args["n_ctx"]
            model_n_batch = private_model_args["n_batch"]
            
            logger.info("Using private model: %s", model_path)

            return GPT4All(
                model=model_path,
                n_ctx=model_n_ctx,
                n_batch=model_n_batch,
                backend="gptj",
                verbose=True,
            )
        else:
            return ChatOpenAI(temperature=0, model_name=model_name)

    def _get_qa(
        self, chat_message: ChatMessage, user_openai_api_key
    ) -> ConversationalRetrievalChain:
        if user_openai_api_key is not None and user_openai_api_key != "":
            self.settings.openai_api_key = user_openai_api_key
        qa = ConversationalRetrievalChain(
            retriever=self.vector_store.as_retriever(),
            max_tokens_limit=chat_message.max_tokens,
            question_generator=self.question_generator,
            combine_docs_chain=self.doc_chain,
            get_chat_history=get_chat_history,
        )
        return qa

    def generate_answer(self, chat_message: ChatMessage, user_openai_api_key) -> str:
        transformed_history = []
        qa = self._get_qa(chat_message, user_openai_api_key)
        for i in range(0, len(chat_message.history) - 1, 2):
            user_message = chat_message.history[i][1]
            assistant_message = chat_message.history[i + 1][1]
            transformed_history.append((user_message, assistant_message))
        model_response = qa(
            {"question": chat_message.question, "chat_history": transformed_history}
        )
        answer = model_response["answer"]
        return answer``` "backend/models/settings.py" ```from typing import Annotated, Any, Dict, List, Tuple, Union
from fastapi import Depends
from langchain.embeddings.openai import OpenAIEmbeddings
from pydantic import BaseSettings
from supabase import Client, create_client
from vectorstore.supabase import SupabaseVectorStore
class BrainSettings(BaseSettings):
    openai_api_key: str
    anthropic_api_key: str
    supabase_url: str
    supabase_service_key: str

class LLMSettings(BaseSettings):
    private: bool
    model_path: str
    model_n_ctx: int
    model_n_batch: int

def common_dependencies() -> dict:
    settings = BrainSettings()
    embeddings = OpenAIEmbeddings(openai_api_key=settings.openai_api_key)
    supabase_client: Client = create_client(settings.supabase_url, settings.supabase_service_key)
    documents_vector_store = SupabaseVectorStore(
    supabase_client, embeddings, table_name="vectors")
    summaries_vector_store = SupabaseVectorStore(
        supabase_client, embeddings, table_name="summaries")
    
    return {
        "supabase": supabase_client,
        "embeddings": embeddings,
        "documents_vector_store": documents_vector_store,
        "summaries_vector_store": summaries_vector_store
    }

CommonsDep = Annotated[dict, Depends(common_dependencies)]```"backend/models/chats.py" ```from typing import List, Optional, Tuple
from uuid import UUID
from pydantic import BaseModel
class ChatMessage(BaseModel):
    model: str = "gpt-3.5-turbo-16k"
    question: str
    history: List[Tuple[str, str]]
    temperature: float = 0.0
    max_tokens: int = 256
    use_summarization: bool = False
    chat_id: Optional[UUID] = None
    chat_name: Optional[str] = None
class ChatAttributes(BaseModel):
    chat_name: Optional[str] = None```"backend/models/settings.py" ```from typing import Annotated, Any, Dict, List, Tuple, Union
from fastapi import Depends
from langchain.embeddings.openai import OpenAIEmbeddings
from pydantic import BaseSettings
from supabase import Client, create_client
from vectorstore.supabase import SupabaseVectorStore
class BrainSettings(BaseSettings):
    openai_api_key: str
    anthropic_api_key: str
    supabase_url: str
    supabase_service_key: str

class LLMSettings(BaseSettings):
    private: bool
    model_path: str
    model_n_ctx: int
    model_n_batch: int

def common_dependencies() -> dict:
    settings = BrainSettings()
    embeddings = OpenAIEmbeddings(openai_api_key=settings.openai_api_key)
    supabase_client: Client = create_client(settings.supabase_url, settings.supabase_service_key)
    documents_vector_store = SupabaseVectorStore(
    supabase_client, embeddings, table_name="vectors")
    summaries_vector_store = SupabaseVectorStore(
        supabase_client, embeddings, table_name="summaries")
    
    return {
        "supabase": supabase_client,
        "embeddings": embeddings,
        "documents_vector_store": documents_vector_store,
        "summaries_vector_store": summaries_vector_store
    }
CommonsDep = Annotated[dict, Depends(common_dependencies)]```"backend/vectorstore/supabase.py" ```from typing import Any, List

from langchain.docstore.document import Document
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import SupabaseVectorStore
from supabase import Client
class CustomSupabaseVectorStore(SupabaseVectorStore):
    user_id: str
    def __init__(self, client: Client, embedding: OpenAIEmbeddings, table_name: str, user_id: str = "none"):
        super().__init__(client, embedding, table_name)
        self.user_id = user_id
    
    def similarity_search(
        self, 
        query: str, 
        table: str = "match_vectors", 
        k: int = 6, 
        threshold: float = 0.5, 
        **kwargs: Any
    ) -> List[Document]:
        vectors = self._embedding.embed_documents([query])
        query_embedding = vectors[0]
        res = self._client.rpc(
            table,
            {
                "query_embedding": query_embedding,
                "match_count": k,
                "p_user_id": self.user_id,
            },
        ).execute()

        match_result = [
            (
                Document(
                    metadata=search.get("metadata", {}),
                    page_content=search.get("content", ""),
                ),
                search.get("similarity", 0.0),
            )
            for search in res.data
            if search.get("content")
        ]

        documents = [doc for doc, _ in match_result]

        return documents``` "uvicorn main:app --host 0.0.0.0 --port 5050           
Traceback (most recent call last):
  File "/quivr/.venv/bin/uvicorn", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/quivr/.venv/lib/python3.11/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/quivr/.venv/lib/python3.11/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
         ^^^^^^^^^^^^^^^^
  File "/quivr/.venv/lib/python3.11/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/quivr/.venv/lib/python3.11/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/quivr/.venv/lib/python3.11/site-packages/uvicorn/main.py", line 410, in main
    run(
  File "/quivr/.venv/lib/python3.11/site-packages/uvicorn/main.py", line 578, in run
    server.run()
  File "/quivr/.venv/lib/python3.11/site-packages/uvicorn/server.py", line 61, in run
    return asyncio.run(self.serve(sockets=sockets))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", line 653, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/quivr/.venv/lib/python3.11/site-packages/uvicorn/server.py", line 68, in serve
    config.load()
  File "/quivr/.venv/lib/python3.11/site-packages/uvicorn/config.py", line 473, in load
    self.loaded_app = import_from_string(self.app)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/quivr/.venv/lib/python3.11/site-packages/uvicorn/importer.py", line 21, in import_from_string
    module = importlib.import_module(module_str)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1206, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1178, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1149, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/quivr/backend/main.py", line 9, in <module>
    from routes.chat_routes import chat_router
  File "/quivr/backend/routes/chat_routes.py", line 6, in <module>
    from llm.brainpicking import BrainPicking
  File "/quivr/backend/llm/brainpicking.py", line 63, in <module>
    class BrainPicking(BaseModel):
  File "/quivr/backend/llm/brainpicking.py", line 72, in BrainPicking
    llm_config = LLMSettings()
                 ^^^^^^^^^^^^^
  File "pydantic/env_settings.py", line 39, in pydantic.env_settings.BaseSettings.__init__
  File "pydantic/main.py", line 341, in pydantic.main.BaseModel.__init__
pydantic.error_wrappers.ValidationError: 4 validation errors for LLMSettings
private
  field required (type=value_error.missing)
model_path
  field required (type=value_error.missing)
model_n_ctx
  field required (type=value_error.missing)
model_n_batch
  field required (type=value_error.missing)". identify the error and give me complete correct modified fastapi python code to resolve this error